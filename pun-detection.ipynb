{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pywsd in ./nlt-project/lib/python3.12/site-packages (1.2.5)\n",
      "Requirement already satisfied: nltk in ./nlt-project/lib/python3.12/site-packages (3.9.1)\n",
      "Requirement already satisfied: transformers in ./nlt-project/lib/python3.12/site-packages (4.44.2)\n",
      "Requirement already satisfied: datasets in ./nlt-project/lib/python3.12/site-packages (3.0.0)\n",
      "Requirement already satisfied: evaluate in ./nlt-project/lib/python3.12/site-packages (0.4.3)\n",
      "Requirement already satisfied: accelerate in ./nlt-project/lib/python3.12/site-packages (0.34.2)\n",
      "Requirement already satisfied: torch in ./nlt-project/lib/python3.12/site-packages (2.4.1)\n",
      "Requirement already satisfied: tensorflow>=2.0.0 in ./nlt-project/lib/python3.12/site-packages (2.17.0)\n",
      "Requirement already satisfied: tensorflow-hub in ./nlt-project/lib/python3.12/site-packages (0.16.1)\n",
      "Requirement already satisfied: scikit-learn in ./nlt-project/lib/python3.12/site-packages (1.5.2)\n",
      "Requirement already satisfied: gensim in ./nlt-project/lib/python3.12/site-packages (4.3.3)\n",
      "Requirement already satisfied: numpy in ./nlt-project/lib/python3.12/site-packages (from pywsd) (1.26.4)\n",
      "Requirement already satisfied: pandas in ./nlt-project/lib/python3.12/site-packages (from pywsd) (2.2.3)\n",
      "Requirement already satisfied: wn==0.0.23 in ./nlt-project/lib/python3.12/site-packages (from pywsd) (0.0.23)\n",
      "Requirement already satisfied: six in ./nlt-project/lib/python3.12/site-packages (from pywsd) (1.16.0)\n",
      "Requirement already satisfied: click in ./nlt-project/lib/python3.12/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in ./nlt-project/lib/python3.12/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in ./nlt-project/lib/python3.12/site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in ./nlt-project/lib/python3.12/site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: filelock in ./nlt-project/lib/python3.12/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in ./nlt-project/lib/python3.12/site-packages (from transformers) (0.25.1)\n",
      "Requirement already satisfied: packaging>=20.0 in ./nlt-project/lib/python3.12/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./nlt-project/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in ./nlt-project/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in ./nlt-project/lib/python3.12/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in ./nlt-project/lib/python3.12/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in ./nlt-project/lib/python3.12/site-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in ./nlt-project/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: xxhash in ./nlt-project/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in ./nlt-project/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in ./nlt-project/lib/python3.12/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in ./nlt-project/lib/python3.12/site-packages (from datasets) (3.10.5)\n",
      "Requirement already satisfied: psutil in ./nlt-project/lib/python3.12/site-packages (from accelerate) (6.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./nlt-project/lib/python3.12/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in ./nlt-project/lib/python3.12/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in ./nlt-project/lib/python3.12/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in ./nlt-project/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: setuptools in ./nlt-project/lib/python3.12/site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in ./nlt-project/lib/python3.12/site-packages (from tensorflow>=2.0.0) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in ./nlt-project/lib/python3.12/site-packages (from tensorflow>=2.0.0) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in ./nlt-project/lib/python3.12/site-packages (from tensorflow>=2.0.0) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in ./nlt-project/lib/python3.12/site-packages (from tensorflow>=2.0.0) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in ./nlt-project/lib/python3.12/site-packages (from tensorflow>=2.0.0) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in ./nlt-project/lib/python3.12/site-packages (from tensorflow>=2.0.0) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in ./nlt-project/lib/python3.12/site-packages (from tensorflow>=2.0.0) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in ./nlt-project/lib/python3.12/site-packages (from tensorflow>=2.0.0) (0.4.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in ./nlt-project/lib/python3.12/site-packages (from tensorflow>=2.0.0) (3.3.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in ./nlt-project/lib/python3.12/site-packages (from tensorflow>=2.0.0) (4.25.5)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in ./nlt-project/lib/python3.12/site-packages (from tensorflow>=2.0.0) (2.4.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in ./nlt-project/lib/python3.12/site-packages (from tensorflow>=2.0.0) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in ./nlt-project/lib/python3.12/site-packages (from tensorflow>=2.0.0) (1.66.1)\n",
      "Requirement already satisfied: tensorboard<2.18,>=2.17 in ./nlt-project/lib/python3.12/site-packages (from tensorflow>=2.0.0) (2.17.1)\n",
      "Requirement already satisfied: keras>=3.2.0 in ./nlt-project/lib/python3.12/site-packages (from tensorflow>=2.0.0) (3.5.0)\n",
      "Requirement already satisfied: tf-keras>=2.14.1 in ./nlt-project/lib/python3.12/site-packages (from tensorflow-hub) (2.17.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in ./nlt-project/lib/python3.12/site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./nlt-project/lib/python3.12/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in ./nlt-project/lib/python3.12/site-packages (from gensim) (7.0.4)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in ./nlt-project/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow>=2.0.0) (0.44.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./nlt-project/lib/python3.12/site-packages (from aiohttp->datasets) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./nlt-project/lib/python3.12/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./nlt-project/lib/python3.12/site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./nlt-project/lib/python3.12/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./nlt-project/lib/python3.12/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./nlt-project/lib/python3.12/site-packages (from aiohttp->datasets) (1.12.1)\n",
      "Requirement already satisfied: rich in ./nlt-project/lib/python3.12/site-packages (from keras>=3.2.0->tensorflow>=2.0.0) (13.8.1)\n",
      "Requirement already satisfied: namex in ./nlt-project/lib/python3.12/site-packages (from keras>=3.2.0->tensorflow>=2.0.0) (0.0.8)\n",
      "Requirement already satisfied: optree in ./nlt-project/lib/python3.12/site-packages (from keras>=3.2.0->tensorflow>=2.0.0) (0.12.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./nlt-project/lib/python3.12/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./nlt-project/lib/python3.12/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./nlt-project/lib/python3.12/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./nlt-project/lib/python3.12/site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: markdown>=2.6.8 in ./nlt-project/lib/python3.12/site-packages (from tensorboard<2.18,>=2.17->tensorflow>=2.0.0) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in ./nlt-project/lib/python3.12/site-packages (from tensorboard<2.18,>=2.17->tensorflow>=2.0.0) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in ./nlt-project/lib/python3.12/site-packages (from tensorboard<2.18,>=2.17->tensorflow>=2.0.0) (3.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./nlt-project/lib/python3.12/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./nlt-project/lib/python3.12/site-packages (from pandas->pywsd) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./nlt-project/lib/python3.12/site-packages (from pandas->pywsd) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./nlt-project/lib/python3.12/site-packages (from pandas->pywsd) (2024.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./nlt-project/lib/python3.12/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./nlt-project/lib/python3.12/site-packages (from rich->keras>=3.2.0->tensorflow>=2.0.0) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./nlt-project/lib/python3.12/site-packages (from rich->keras>=3.2.0->tensorflow>=2.0.0) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./nlt-project/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow>=2.0.0) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pywsd nltk transformers datasets evaluate accelerate torch \"tensorflow>=2.0.0\" --upgrade tensorflow-hub scikit-learn gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to /Users/vivekk/nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/vivekk/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/vivekk/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/vivekk/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /Users/vivekk/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('reuters')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets\n",
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "\n",
    "# Transformers\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer, AutoTokenizer, DataCollatorWithPadding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sklearn for data splitting\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLP and evaluation tools\n",
    "from nltk.corpus import stopwords, reuters, wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General utilities\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation library\n",
    "import evaluate\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "pun_dataset = load_dataset(\"CreativeLang/pun_detection_semeval2017_task7\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>type</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hom_1</td>\n",
       "      <td>1</td>\n",
       "      <td>homographic</td>\n",
       "      <td>They hid from the gunman in a sauna where they...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hom_2</td>\n",
       "      <td>1</td>\n",
       "      <td>homographic</td>\n",
       "      <td>Wal - Mart isn't the only saving place !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hom_3</td>\n",
       "      <td>1</td>\n",
       "      <td>homographic</td>\n",
       "      <td>Can honeybee abuse lead to a sting operation ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hom_4</td>\n",
       "      <td>1</td>\n",
       "      <td>homographic</td>\n",
       "      <td>A ditch digger was entrenched in his career .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hom_5</td>\n",
       "      <td>1</td>\n",
       "      <td>homographic</td>\n",
       "      <td>She was only a Blacksmith's daughter , but she...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  label         type  \\\n",
       "0  hom_1      1  homographic   \n",
       "1  hom_2      1  homographic   \n",
       "2  hom_3      1  homographic   \n",
       "3  hom_4      1  homographic   \n",
       "4  hom_5      1  homographic   \n",
       "\n",
       "                                                text  \n",
       "0  They hid from the gunman in a sauna where they...  \n",
       "1           Wal - Mart isn't the only saving place !  \n",
       "2     Can honeybee abuse lead to a sting operation ?  \n",
       "3      A ditch digger was entrenched in his career .  \n",
       "4  She was only a Blacksmith's daughter , but she...  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_dataset = pun_dataset['train'].filter(lambda example: example['type'] == 'homographic')\n",
    "df = pd.DataFrame(filtered_dataset)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf, testdf = train_test_split(df, test_size=0.2)\n",
    "trainds = Dataset.from_pandas(traindf)\n",
    "testds = Dataset.from_pandas(testdf)\n",
    "\n",
    "ds = DatasetDict()\n",
    "ds['train'] = trainds\n",
    "ds['test'] = testds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vivekk/Desktop/NLT-Project-1/nlt-project/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def preprocess_function(input_data):\n",
    "  return tokenizer(input_data[\"text\"], truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92c8a6b3d1e94f2aa8389c01ff8b552e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "876410ada64d49cf8bc9c3fb400a5a52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/450 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_data = ds.map(preprocess_function, batched = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {0: \"NEGATIVE\", 1: \"POSITIVE\"}\n",
    "label2id = {\"NEGATIVE\": 0, \"POSITIVE\": 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\", num_labels=2, id2label=id2label, label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vivekk/Desktop/NLT-Project-1/nlt-project/lib/python3.12/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./distilbert_finetuned_puns\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_data['train'],\n",
    "    eval_dataset=tokenized_data['test'],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6206320f7f0b4ebe894b9f0c316f77db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/226 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d91500668983443cb08bc1d07818fcbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2571030557155609, 'eval_accuracy': 0.9022222222222223, 'eval_runtime': 3.9296, 'eval_samples_per_second': 114.516, 'eval_steps_per_second': 7.38, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e20eb58cb9c4b05aa7ce10f9c7a5bff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.22389210760593414, 'eval_accuracy': 0.9177777777777778, 'eval_runtime': 3.16, 'eval_samples_per_second': 142.407, 'eval_steps_per_second': 9.177, 'epoch': 2.0}\n",
      "{'train_runtime': 164.8126, 'train_samples_per_second': 21.843, 'train_steps_per_second': 1.371, 'train_loss': 0.28668486333526344, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=226, training_loss=0.28668486333526344, metrics={'train_runtime': 164.8126, 'train_samples_per_second': 21.843, 'train_steps_per_second': 1.371, 'total_flos': 24663773787264.0, 'train_loss': 0.28668486333526344, 'epoch': 2.0})"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pun_detection(input_sentence):\n",
    "      inputs = tokenizer(input_sentence, return_tensors=\"pt\")\n",
    "\n",
    "      # Check if GPU (CUDA) is available\n",
    "      device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "      # Move your model to the device\n",
    "      model.to(device)\n",
    "\n",
    "      # Move your input tensor to the same device\n",
    "      inputs = inputs.to(device)\n",
    "      # Step 2: Pass the tokenized input through the model\n",
    "      with torch.no_grad():\n",
    "          outputs = model(**inputs)\n",
    "\n",
    "      # The 'outputs' variable contains the model's predictions\n",
    "      logits = outputs.logits\n",
    "      # If you have binary classification, you can apply a softmax to get probabilities\n",
    "      # If you have more than two labels, you can use a softmax for multiclass classification\n",
    "      # For binary classification:\n",
    "      probs = torch.softmax(logits, dim=1)\n",
    "\n",
    "      # To get the predicted class labels for binary classification (0 or 1), you can use argmax\n",
    "      predicted_class = torch.argmax(probs, dim=1).item()\n",
    "\n",
    "      # For multiclass classification, you can get the class labels directly\n",
    "      predicted_labels = torch.argmax(logits, dim=1).tolist()\n",
    "\n",
    "      # If you have label-to-class mapping (id2label), you can map the labels back to their original labels\n",
    "      predicted_labels = [id2label[label] for label in predicted_labels]\n",
    "\n",
    "      return predicted_class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pun_detection(\"I used to be a baker, but I couldn't make enough dough.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pun_detection(\"Why do elephants have a trunk? Because they don’t have pockets to put stuff in.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pun Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.update('.', '?', '-', \"'\", ':', ',', '!', '<', '>', '\"', '/', '(', ')',\n",
    "                      '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 's', 't', 're', 'm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_pun_word(entry):\n",
    "    sentence = [word for word in entry.split() if word.lower() not in stop_words]\n",
    "    rand_pun_number = random.randint(0, len(sentence) - 1)\n",
    "    predicted_word = sentence[rand_pun_number]\n",
    "    return predicted_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last_word(entry):\n",
    "    sentence = [word for word in entry.split() if word.lower() not in stop_words]\n",
    "    if sentence:\n",
    "        last_word = sentence[-1]\n",
    "    return last_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_with_most_senses(entry):\n",
    "    senses_number = 0\n",
    "    prev_word = \"\"\n",
    "    sentence = [word for word in entry.split() if word.lower() not in stop_words]\n",
    "    for word in sentence:\n",
    "        word_senses_num = len(wn.synsets(word.lower()))  # Count number of senses in WordNet\n",
    "        if word_senses_num >= senses_number:  # Track word with most senses\n",
    "            senses_number = word_senses_num\n",
    "            prev_word = word\n",
    "    # Return the word with the most senses, or an empty string if no word was found\n",
    "    return prev_word if prev_word else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_pun_detection_accuracy(json_file_path, find_pun_function):\n",
    "    sentences = []\n",
    "    puns = []\n",
    "    predicted = []\n",
    "    \n",
    "    # Load the JSON file\n",
    "    with open(json_file_path, 'r') as json_file:\n",
    "        data = json.load(json_file)\n",
    "\n",
    "    # Extract sentences and puns\n",
    "    for item in data:\n",
    "        sentence = item.get('sentence')\n",
    "        pun = item.get('src')  # Assuming 'src' contains the pun word\n",
    "        sentences.append(sentence)\n",
    "        puns.append(pun)\n",
    "\n",
    "    # Use the provided function to predict pun words\n",
    "    for sentence in sentences:\n",
    "        pun_word = find_pun_function(sentence)\n",
    "        predicted.append(pun_word)\n",
    "\n",
    "    # Check if both lists have the same length\n",
    "    total_puns = len(puns)\n",
    "    total_predicted = len(predicted)\n",
    "\n",
    "    if total_puns != total_predicted:\n",
    "        print(\"Error: Different length between actual and predicted puns.\")\n",
    "        return None\n",
    "\n",
    "    # Calculate accuracy\n",
    "    correct_count = sum(1 for i in range(total_puns) if puns[i] == predicted[i])\n",
    "    accuracy = (correct_count / total_puns) * 100\n",
    "\n",
    "    # Print the function name used for prediction\n",
    "    print(f\"Function used for prediction: {find_pun_function.__name__}\")\n",
    "    \n",
    "    # Print the accuracy\n",
    "    print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "# Example of using the function with a sample pun prediction function\n",
    "def sample_find_pun_function(sentence):\n",
    "    # Placeholder pun detection logic (you can replace this with your actual logic)\n",
    "    words = sentence.split()\n",
    "    return words[-1]  # Example: Just returning the last word as a pun (replace with your logic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2vmodel = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sentence(sentence):\n",
    "    words = sentence.lower().split()\n",
    "    return [word for word in words if word not in stop_words and word in w2vmodel.key_to_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_possible_pun_words(sent):\n",
    "    scores = {}\n",
    "    if len(sent) <= 1:\n",
    "        return set(sent[0])\n",
    "    else:\n",
    "        for i in range(len(sent)-1):\n",
    "            for j in range(i+1, len(sent)):\n",
    "                sim_score = w2vmodel.similarity(sent[i], sent[j])\n",
    "                scores['{0}-{1}'.format(sent[i], sent[j])] = sim_score\n",
    "\n",
    "        if len(scores) >= 5:\n",
    "            top3 = sorted(zip(scores.values(), scores.keys()), reverse=True)[:3]\n",
    "            poss = [tup[1].split(sep='-') for tup in top3]\n",
    "            possible_pun_words = set(poss[0] + poss[1] + poss[2])\n",
    "        else:\n",
    "            poss = [pair.split(sep='-') for pair in scores.keys()]\n",
    "            possible_pun_words = set()\n",
    "            for i in range(len(poss)):\n",
    "                possible_pun_words = possible_pun_words.union(set(poss[i]))\n",
    "            \n",
    "        return possible_pun_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_pun_w2v(sentence):\n",
    "    # Clean the sentence\n",
    "    clean_sent = clean_sentence(sentence)\n",
    "    \n",
    "    if not clean_sent:\n",
    "        return None  # No valid words to process\n",
    "\n",
    "    # Get possible pun words\n",
    "    possible_pun_words = get_possible_pun_words(clean_sent)\n",
    "\n",
    "    # Tokenize the original sentence\n",
    "    original_words = sentence.lower().split()\n",
    "\n",
    "    # Filter possible pun words to include only those that exist in the original sentence\n",
    "    possible_pun_words_in_original = [w for w in possible_pun_words if w in original_words]\n",
    "\n",
    "    # Check if there are valid pun words in the original sentence\n",
    "    if not possible_pun_words_in_original:\n",
    "        return None  # No pun word found\n",
    "\n",
    "    # Find the pun word with the highest index (appearing last in the sentence)\n",
    "    pun_word = max(possible_pun_words_in_original, key=lambda w: original_words.index(w))\n",
    "    \n",
    "    return pun_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = reuters.fileids()\n",
    "word_frequencies = Counter()\n",
    "\n",
    "for doc_id in documents:\n",
    "    words = reuters.words(doc_id)\n",
    "    word_frequencies.update(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word frequency\n",
    "def calculate_frequency(word):\n",
    "    freq = word_frequencies[word]\n",
    "    return freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NPMI\n",
    "def calculate_npmi(sentence, word, content_word, total_words):\n",
    "    words = nltk.word_tokenize(sentence.lower())\n",
    "    # Calculate the number of appearances of word and content_word\n",
    "    word_count = sum(1 for w in words if w == word)\n",
    "    content_word_count = sum(1 for w in words if w == content_word)\n",
    "    co_occurrence_count = sum(1 for i in range(len(words) - 1) if words[i] == word and words[i + 1] == content_word)\n",
    "\n",
    "    # Calculate the probability of each word\n",
    "    p_word = word_count / total_words\n",
    "    p_content_word = content_word_count / total_words\n",
    "    p_co_occurrence = co_occurrence_count / (total_words - 1)  # 1 less than the length of the sentence\n",
    "\n",
    "    # Calculate NPMI\n",
    "    if p_word == 0.0 or p_content_word == 0.0 or p_co_occurrence == 0.0:\n",
    "        return 0.0\n",
    "\n",
    "    pmi = math.log2(p_co_occurrence / (p_word * p_content_word))\n",
    "    npmi = pmi / (-math.log2(p_co_occurrence))\n",
    "\n",
    "    return npmi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines the word class tagging function\n",
    "def get_word_pos(word):\n",
    "    pos = nltk.pos_tag([word])[0][1]\n",
    "    if pos.startswith('N'):  # Noun\n",
    "        return 'n'\n",
    "    elif pos.startswith('J'):  # Adjective\n",
    "        return 'a'\n",
    "    elif pos.startswith('R'):  # Adverb\n",
    "        return 'r'\n",
    "    elif pos.startswith('V'):  # Verb\n",
    "        return 'v'\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get word frequency information to use in a sentence\n",
    "def get_word_frequencies(sentence):\n",
    "    words = nltk.word_tokenize(sentence.lower())\n",
    "    word_freq = Counter(words)\n",
    "    return words, word_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find pun in a sentence\n",
    "def find_pun(sentence):\n",
    "    words, word_freq = get_word_frequencies(sentence)  # Get word list and frequency information\n",
    "    total_words = len(words)\n",
    "\n",
    "    highest_score = -1\n",
    "    pun_word = None\n",
    "\n",
    "    # Calculate and evaluate scores for each word in a sentence\n",
    "    for i, word in enumerate(word_freq.keys()):\n",
    "        score = 0\n",
    "        #only consider the words that are noun, adj, adverb or verb\n",
    "        if get_word_pos(word) in ['n', 'a', 'r', 'v']:\n",
    "          # 1. +1 points for words that are not in the corpus\n",
    "          freq = calculate_frequency(word)\n",
    "          if freq==0:\n",
    "            score += 1\n",
    "\n",
    "          # 2. Calculate the number of content words with NPMI(x, y) > 0.3\n",
    "          content_words = [w for w in word_freq.keys() if w != word and w not in stopwords.words('english')]\n",
    "          nmpi_count = sum(1 for content_word in content_words if calculate_npmi(sentence, word, content_word, total_words) > 0.3 and len(wordnet.synsets(word))>1)\n",
    "          score += nmpi_count\n",
    "\n",
    "          # 3. Give points to words located in 3/4 and 4/4\n",
    "          if i >= total_words * 3 / 4:\n",
    "              score += 2\n",
    "          elif i >= total_words * 1 / 2:\n",
    "              score += 1\n",
    "\n",
    "          # 4. Score the words that appear after the first comma in a sentence\n",
    "          if ',' in sentence[:i]:\n",
    "              score += 1\n",
    "\n",
    "          # 5. Score words that appear after first \"then\" in a sentence\n",
    "          if 'then' in sentence[:i]:\n",
    "              score += 1\n",
    "\n",
    "          # 6. Score words that appear after first \"but\" in a sentence\n",
    "          if 'but' in sentence[:i]:\n",
    "              score += 1\n",
    "\n",
    "          # 7. Bonus for having the highest IDF among words located in the second half (after three quarters)\n",
    "          if i >= total_words * 3 / 4:\n",
    "              second_half_words = list(word_freq.keys())[i:]\n",
    "              idf_scores = {}\n",
    "              for w in second_half_words:\n",
    "                  idf = math.log(total_words / (word_freq.get(w, 1) + 1))\n",
    "                  idf_scores[w] = idf\n",
    "              max_idf_word = max(idf_scores, key=idf_scores.get)\n",
    "              if word == max_idf_word:\n",
    "                  score += 1\n",
    "\n",
    "          # Update highest score and pun word\n",
    "          if score > highest_score:\n",
    "              highest_score = score\n",
    "              pun_word = word\n",
    "        else: continue\n",
    "    return pun_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dough'"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_pun(\"I used to be a baker, but I couldn’t make enough dough.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'interest'"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_pun(\"I used to be a banker, but I lost interest.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'problems'"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_pun(\"I used to be a math teacher, but I found it too difficult to solve my problems.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'charge'"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_pun(\"I gave all my dead batteries away today… free of charge.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'never'"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_pun(\"He had a photographic memory but never developed it.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'chicago'"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_pun(\"The band is going to wind up their tour in Chicago.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function used for prediction: get_random_pun_word\n",
      "Accuracy: 17.87%\n",
      "Function used for prediction: get_word_with_most_senses\n",
      "Accuracy: 26.58%\n",
      "Function used for prediction: get_last_word\n",
      "Accuracy: 61.17%\n",
      "Function used for prediction: find_pun_w2v\n",
      "Accuracy: 48.00%\n",
      "Function used for prediction: find_pun\n",
      "Accuracy: 50.08%\n"
     ]
    }
   ],
   "source": [
    "json_file_path = 'semeval-task3-homo.json'\n",
    "calculate_pun_detection_accuracy(json_file_path, get_random_pun_word)\n",
    "calculate_pun_detection_accuracy(json_file_path, get_word_with_most_senses)\n",
    "calculate_pun_detection_accuracy(json_file_path, get_last_word)\n",
    "calculate_pun_detection_accuracy(json_file_path, find_pun_w2v)\n",
    "calculate_pun_detection_accuracy(json_file_path, find_pun)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pun Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained ELMo model\n",
    "elmo = hub.KerasLayer(\"https://tfhub.dev/google/elmo/3\", trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_second_closest_number(numbers, target):\n",
    "    if len(numbers) < 2:\n",
    "        raise ValueError(\"The list must contain at least two numbers.\")\n",
    "    \n",
    "    closest_number = numbers[0]\n",
    "    second_closest_number = numbers[1]\n",
    "\n",
    "    for number in numbers:\n",
    "        diff1 = abs(number - target)\n",
    "        diff2 = abs(second_closest_number - target)\n",
    "        diff3 = abs(closest_number - target)\n",
    "\n",
    "        if diff1 < diff3:\n",
    "            second_closest_number = closest_number\n",
    "            closest_number = number\n",
    "        elif diff1 < diff2 and number != closest_number:\n",
    "            second_closest_number = number\n",
    "\n",
    "    return second_closest_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_numbers(numbers, target):\n",
    "    vicinity_values = [number for number in numbers if int(number * 10) == int(target * 10)]\n",
    "    if not vicinity_values:\n",
    "        target = (target * 10 - 10) / 10\n",
    "        return find_numbers(numbers, target)\n",
    "    return vicinity_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def find_farthest_value(numbers, target):\n",
    "    farthest_value = None\n",
    "    max_distance = float('-inf')  # Initialize with negative infinity\n",
    "\n",
    "    for number in numbers:\n",
    "        distance = abs(number - target)\n",
    "        if distance > max_distance:\n",
    "            max_distance = distance\n",
    "            farthest_value = number\n",
    "\n",
    "    return farthest_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pywsd.lesk import adapted_lesk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pun_interpretation(sentence, pun):\n",
    "    # Find the index of the pun word\n",
    "    sentence_words = sentence.split(\" \")\n",
    "    index = sentence_words.index(pun)\n",
    "\n",
    "    # Use the adapted_lesk function from pywsd for word sense disambiguation\n",
    "    sense1 = adapted_lesk(sentence, pun)\n",
    "    \n",
    "    if sense1:\n",
    "        senses = wordnet.synsets(pun)\n",
    "        \n",
    "        # Get the definitions of the synsets\n",
    "        sen_def = [i.definition() for i in senses]\n",
    "\n",
    "        # Building word vectors for the target word from the test dataset\n",
    "        ind_embeddings = elmo(tf.constant([sentence]))\n",
    "        target_vector = ind_embeddings[0][index].numpy()\n",
    "\n",
    "        ind_synset_embeddings = elmo(tf.constant(sen_def))\n",
    "        synset_vectors = [tf.reduce_mean(ind_synset_embeddings[i], axis=0).numpy() for i in range(len(senses))]\n",
    "\n",
    "        cosine = []\n",
    "        for i in range(len(senses)):\n",
    "            dot_product = tf.reduce_sum(tf.multiply(target_vector, synset_vectors[i]))\n",
    "            magnitude1 = tf.sqrt(tf.reduce_sum(tf.square(target_vector)))\n",
    "            magnitude2 = tf.sqrt(tf.reduce_sum(tf.square(synset_vectors[i])))\n",
    "            cosine_similarity = dot_product / (magnitude1 * magnitude2)\n",
    "            cosine.append(cosine_similarity.numpy())\n",
    "\n",
    "        # Find the index of sense1 in the list of synsets, not just the definitions\n",
    "        if sense1 in senses:\n",
    "            k = senses.index(sense1)\n",
    "        else:\n",
    "            print(f\"Warning: Sense from adapted_lesk not found in WordNet synsets for '{pun}'\")\n",
    "            return None\n",
    "\n",
    "        if len(cosine) < 2:\n",
    "            return None\n",
    "        \n",
    "        # Find second closest sense\n",
    "        p = find_second_closest_number(cosine, cosine[k])\n",
    "        sense2 = senses[cosine.index(p)]\n",
    "\n",
    "        # Find farthest sense (optional, if required)\n",
    "        vic = find_farthest_value(find_numbers(cosine, cosine[k]), cosine[k])\n",
    "        farthest_sense = senses[cosine.index(vic)]\n",
    "\n",
    "        # Return the two senses (sense1 and sense2)\n",
    "        return {\"sense1\": sense1.definition(), \"sense2\": sense2.definition()}\n",
    "    else:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    # Remove punctuation\n",
    "    sentence = ''.join([char for char in sentence if char not in string.punctuation])\n",
    "\n",
    "    # Tokenize the sentence into words\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "\n",
    "    # Remove stopwords\n",
    "    words = [word for word in words if word.lower() not in stopwords.words(\"english\")]\n",
    "\n",
    "    # Rejoin the words to form a cleaned sentence\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pun_complete_detection(sentence):\n",
    "    if pun_detection(sentence) == 1:\n",
    "        print(\"Pun detected!\")\n",
    "        cleaned_sentence = preprocess_sentence(sentence)\n",
    "        pun_word = find_pun(cleaned_sentence)\n",
    "        print(\"Pun word:\", pun_word)\n",
    "        print(pun_interpretation(cleaned_sentence, pun_word))\n",
    "    else:\n",
    "        print(\"No pun detected in the sentence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pun detected!\n",
      "Pun word: sweat\n",
      "{'sense1': 'excrete perspiration through the pores in the skin', 'sense2': 'salty fluid secreted by sweat glands'}\n",
      "-----------------------------------\n",
      "No pun detected in the sentence\n",
      "-----------------------------------\n",
      "Pun detected!\n",
      "Pun word: drill\n",
      "{'sense1': 'systematic training by multiple repetitions', 'sense2': 'a tool with a sharp point and cutting edges for making holes in hard materials (usually rotating rapidly or by repeated blows)'}\n",
      "-----------------------------------\n",
      "Pun detected!\n",
      "Pun word: match\n",
      "{'sense1': 'a formal contest in which two or more persons or teams compete', 'sense2': 'lighter consisting of a thin piece of wood or cardboard tipped with combustible chemical; ignites with friction'}\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "sentences = [\"They hid from the gunman in a sauna where they could sweat it out.\",\"\",\n",
    "                     \"I've been to the dentist so many times, I really know the drill.\",\n",
    "                     \"A contest held by fire fighters is called a ' match '.\"\n",
    "                     ]\n",
    "for sentence in sentences:\n",
    "  pun_complete_detection(sentence)\n",
    "  print(\"-----------------------------------\")\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence_transformers in ./nlt-project/lib/python3.12/site-packages (3.1.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.38.0 in ./nlt-project/lib/python3.12/site-packages (from sentence_transformers) (4.44.2)\n",
      "Requirement already satisfied: tqdm in ./nlt-project/lib/python3.12/site-packages (from sentence_transformers) (4.66.5)\n",
      "Requirement already satisfied: torch>=1.11.0 in ./nlt-project/lib/python3.12/site-packages (from sentence_transformers) (2.4.1)\n",
      "Requirement already satisfied: scikit-learn in ./nlt-project/lib/python3.12/site-packages (from sentence_transformers) (1.5.2)\n",
      "Requirement already satisfied: scipy in ./nlt-project/lib/python3.12/site-packages (from sentence_transformers) (1.13.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.3 in ./nlt-project/lib/python3.12/site-packages (from sentence_transformers) (0.25.1)\n",
      "Requirement already satisfied: Pillow in ./nlt-project/lib/python3.12/site-packages (from sentence_transformers) (10.4.0)\n",
      "Requirement already satisfied: filelock in ./nlt-project/lib/python3.12/site-packages (from huggingface-hub>=0.19.3->sentence_transformers) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./nlt-project/lib/python3.12/site-packages (from huggingface-hub>=0.19.3->sentence_transformers) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in ./nlt-project/lib/python3.12/site-packages (from huggingface-hub>=0.19.3->sentence_transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./nlt-project/lib/python3.12/site-packages (from huggingface-hub>=0.19.3->sentence_transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in ./nlt-project/lib/python3.12/site-packages (from huggingface-hub>=0.19.3->sentence_transformers) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./nlt-project/lib/python3.12/site-packages (from huggingface-hub>=0.19.3->sentence_transformers) (4.12.2)\n",
      "Requirement already satisfied: sympy in ./nlt-project/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers) (1.13.3)\n",
      "Requirement already satisfied: networkx in ./nlt-project/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers) (3.3)\n",
      "Requirement already satisfied: jinja2 in ./nlt-project/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers) (3.1.4)\n",
      "Requirement already satisfied: setuptools in ./nlt-project/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers) (75.1.0)\n",
      "Requirement already satisfied: numpy>=1.17 in ./nlt-project/lib/python3.12/site-packages (from transformers<5.0.0,>=4.38.0->sentence_transformers) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./nlt-project/lib/python3.12/site-packages (from transformers<5.0.0,>=4.38.0->sentence_transformers) (2024.9.11)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in ./nlt-project/lib/python3.12/site-packages (from transformers<5.0.0,>=4.38.0->sentence_transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in ./nlt-project/lib/python3.12/site-packages (from transformers<5.0.0,>=4.38.0->sentence_transformers) (0.19.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./nlt-project/lib/python3.12/site-packages (from scikit-learn->sentence_transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./nlt-project/lib/python3.12/site-packages (from scikit-learn->sentence_transformers) (3.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./nlt-project/lib/python3.12/site-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./nlt-project/lib/python3.12/site-packages (from requests->huggingface-hub>=0.19.3->sentence_transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./nlt-project/lib/python3.12/site-packages (from requests->huggingface-hub>=0.19.3->sentence_transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./nlt-project/lib/python3.12/site-packages (from requests->huggingface-hub>=0.19.3->sentence_transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./nlt-project/lib/python3.12/site-packages (from requests->huggingface-hub>=0.19.3->sentence_transformers) (2024.8.30)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./nlt-project/lib/python3.12/site-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vivekk/Desktop/NLT-Project-1/nlt-project/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "stmodel = SentenceTransformer('all-mpnet-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_top_senses(sentence, pun_word, search_size):\n",
    "    # Generate sentence embedding\n",
    "    sentence_embedding = stmodel.encode(sentence, convert_to_tensor=True)\n",
    "\n",
    "    # Retrieve WordNet senses for the pun word\n",
    "    synsets = wn.synsets(pun_word)\n",
    "\n",
    "    # Compute embeddings for each sense's definition or lemma name\n",
    "    sense_embeddings = []\n",
    "    sense_data = []  # Store both lemma and synset\n",
    "\n",
    "    for synset in synsets:\n",
    "        # Get all synonyms (lemma names) for each sense\n",
    "        for lemma in synset.lemma_names():\n",
    "            # Encode the lemma or definition of the synset\n",
    "            lemma_embedding = stmodel.encode(lemma, convert_to_tensor=True)\n",
    "            sense_embeddings.append(lemma_embedding)\n",
    "            sense_data.append((lemma, synset))  # Store lemma and synset together\n",
    "\n",
    "    # Compute cosine similarities between sentence embedding and sense embeddings\n",
    "    similarities = util.pytorch_cos_sim(sentence_embedding, torch.stack(sense_embeddings))\n",
    "\n",
    "    # Sort the similarities in descending order and extract the top two scores\n",
    "    sorted_indices = similarities.argsort(descending=True)[0]\n",
    "    top_n_similarities = similarities[0][sorted_indices[:search_size]]  # Top two similarities\n",
    "\n",
    "    # Collect senses with the same similarity as the second highest score\n",
    "    selected_senses = []\n",
    "    nth_highest_score = top_n_similarities[search_size-1].item()  # Second highest similarity\n",
    "\n",
    "    for idx in sorted_indices:\n",
    "        sim_score = similarities[0][idx].item()\n",
    "        if sim_score >= nth_highest_score:\n",
    "            selected_senses.append((sense_data[idx][0], sense_data[idx][1].definition(), sim_score))\n",
    "    return selected_senses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpret_pun(sentence, pun_word, search_size = 2):\n",
    "    selected_senses = select_top_senses(sentence, pun_word, search_size)\n",
    "    # Display senses and similarity scores\n",
    "    print(f\"Selected senses for the word '{pun_word}':\")\n",
    "    for lemma, definition, similarity in selected_senses:\n",
    "        print(f\"{lemma} (Similarity score: {similarity:.4f}) : {definition}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pun_complete_detection_st(sentence):\n",
    "    if pun_detection(sentence) == 1:\n",
    "        print(\"Pun detected!\")\n",
    "        cleaned_sentence = preprocess_sentence(sentence)\n",
    "        pun_word = find_pun(cleaned_sentence)\n",
    "        print(\"Pun word:\", pun_word)\n",
    "        print(interpret_pun(cleaned_sentence, pun_word,2))\n",
    "    else:\n",
    "        print(\"No pun detected in the sentence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pun detected!\n",
      "Pun word: sweat\n",
      "Selected senses for the word 'sweat':\n",
      "sweat (Similarity score: 0.5356) : salty fluid secreted by sweat glands\n",
      "sweat (Similarity score: 0.5356) : agitation resulting from active worry\n",
      "sweat (Similarity score: 0.5356) : condensation of moisture on a cold surface\n",
      "sweat (Similarity score: 0.5356) : use of physical or mental energy; hard work\n",
      "sweat (Similarity score: 0.5356) : excrete perspiration through the pores in the skin\n",
      "None\n",
      "-----------------------------------\n",
      "Pun detected!\n",
      "Pun word: drill\n",
      "Selected senses for the word 'drill':\n",
      "drill (Similarity score: 0.6715) : a tool with a sharp point and cutting edges for making holes in hard materials (usually rotating rapidly or by repeated blows)\n",
      "drill (Similarity score: 0.6715) : similar to the mandrill but smaller and less brightly colored\n",
      "drill (Similarity score: 0.6715) : systematic training by multiple repetitions\n",
      "drill (Similarity score: 0.6715) : (military) the training of soldiers to march (as in ceremonial parades) or to perform the manual of arms\n",
      "drill (Similarity score: 0.6715) : make a hole, especially with a pointed power or hand tool\n",
      "drill (Similarity score: 0.6715) : train in the military, e.g., in the use of weapons\n",
      "drill (Similarity score: 0.6715) : learn by repetition\n",
      "drill (Similarity score: 0.6715) : teach by repetition\n",
      "drill (Similarity score: 0.6715) : undergo military training or do military exercises\n",
      "None\n",
      "-----------------------------------\n",
      "Pun detected!\n",
      "Pun word: match\n",
      "Selected senses for the word 'match':\n",
      "match (Similarity score: 0.5600) : lighter consisting of a thin piece of wood or cardboard tipped with combustible chemical; ignites with friction\n",
      "match (Similarity score: 0.5600) : a formal contest in which two or more persons or teams compete\n",
      "match (Similarity score: 0.5600) : a burning piece of wood or cardboard\n",
      "match (Similarity score: 0.5600) : an exact duplicate\n",
      "match (Similarity score: 0.5600) : the score needed to win a match\n",
      "match (Similarity score: 0.5600) : a person regarded as a good matrimonial prospect\n",
      "match (Similarity score: 0.5600) : a person who is of equal standing with another in a group\n",
      "match (Similarity score: 0.5600) : a pair of people who live together\n",
      "match (Similarity score: 0.5600) : something that resembles or harmonizes with\n",
      "match (Similarity score: 0.5600) : be compatible, similar or consistent; coincide in their characteristics\n",
      "match (Similarity score: 0.5600) : provide funds complementary to\n",
      "match (Similarity score: 0.5600) : bring two objects, ideas, or people together\n",
      "match (Similarity score: 0.5600) : be equal to in quality or ability\n",
      "match (Similarity score: 0.5600) : make correspond or harmonize\n",
      "match (Similarity score: 0.5600) : satisfy or fulfill\n",
      "match (Similarity score: 0.5600) : give or join in marriage\n",
      "match (Similarity score: 0.5600) : set into opposition or rivalry\n",
      "match (Similarity score: 0.5600) : be equal or harmonize\n",
      "match (Similarity score: 0.5600) : make equal, uniform, corresponding, or matching\n",
      "None\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "sentences = [\"They hid from the gunman in a sauna where they could sweat it out.\",\n",
    "                     \"I've been to the dentist so many times, I really know the drill.\",\n",
    "                     \"A contest held by fire fighters is called a ' match '.\"\n",
    "                     ]\n",
    "for sentence in sentences:\n",
    "  pun_complete_detection_st(sentence)\n",
    "  print(\"-----------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in ./nlt-project/lib/python3.12/site-packages (1.47.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in ./nlt-project/lib/python3.12/site-packages (from openai) (4.6.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./nlt-project/lib/python3.12/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./nlt-project/lib/python3.12/site-packages (from openai) (0.27.2)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in ./nlt-project/lib/python3.12/site-packages (from openai) (0.5.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in ./nlt-project/lib/python3.12/site-packages (from openai) (2.9.2)\n",
      "Requirement already satisfied: sniffio in ./nlt-project/lib/python3.12/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in ./nlt-project/lib/python3.12/site-packages (from openai) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in ./nlt-project/lib/python3.12/site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in ./nlt-project/lib/python3.12/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in ./nlt-project/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in ./nlt-project/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./nlt-project/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./nlt-project/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in ./nlt-project/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (2.23.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_senses_with_llm(sentence, pun_word, senses):\n",
    "    prompt = f\"\"\"\n",
    "    The word \"{pun_word}\" in the sentence \"{sentence}\" has multiple meanings.\n",
    "    The following interpretations were chosen based on context:\n",
    "    \"\"\"\n",
    "\n",
    "    for i, (lemma, definition, score) in enumerate(senses):\n",
    "        prompt += f\"{i+1}. {lemma} - {definition}\\n\"\n",
    "\n",
    "    prompt += \"Select the best two interpretations that fit the context of the sentence.\"\n",
    "\n",
    "    completion = openai.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            },\n",
    "        ],\n",
    "    )\n",
    "    return completion.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpret_pun_with_gpt(sentence, pun_word, search_size = 2):\n",
    "    selected_senses = select_top_senses(sentence, pun_word, search_size)\n",
    "    # Display senses and similarity scores\n",
    "    print(f\"Selected senses for the word '{pun_word}':\")\n",
    "    for lemma, definition, similarity in selected_senses:\n",
    "        print(f\"{lemma} (Similarity score: {similarity:.4f}) : {definition}\")\n",
    "\n",
    "    # Check if we need to use the LLM\n",
    "    if len(selected_senses) > 2:\n",
    "        print(\"\\nSending multiple senses to LLM for verification...\")\n",
    "        llm_response = verify_senses_with_llm(sentence, pun_word, selected_senses)\n",
    "        print(\"\\nLLM's feedback on the chosen senses:\")\n",
    "        print(llm_response)\n",
    "    else:\n",
    "        print(\"\\nNo need for LLM verification. Only two senses selected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pun_complete_detection_st_gpt(sentence):\n",
    "    if pun_detection(sentence) == 1:\n",
    "        print(\"Pun detected!\")\n",
    "        cleaned_sentence = preprocess_sentence(sentence)\n",
    "        pun_word = find_pun(cleaned_sentence)\n",
    "        print(\"Pun word:\", pun_word)\n",
    "        print(interpret_pun_with_gpt(cleaned_sentence, pun_word,2))\n",
    "    else:\n",
    "        print(\"No pun detected in the sentence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pun detected!\n",
      "Pun word: sweat\n",
      "Selected senses for the word 'sweat':\n",
      "sweat (Similarity score: 0.5356) : salty fluid secreted by sweat glands\n",
      "sweat (Similarity score: 0.5356) : agitation resulting from active worry\n",
      "sweat (Similarity score: 0.5356) : condensation of moisture on a cold surface\n",
      "sweat (Similarity score: 0.5356) : use of physical or mental energy; hard work\n",
      "sweat (Similarity score: 0.5356) : excrete perspiration through the pores in the skin\n",
      "\n",
      "Sending multiple senses to LLM for verification...\n",
      "\n",
      "LLM's feedback on the chosen senses:\n",
      "1. sweat - salty fluid secreted by sweat glands\n",
      "3. sweat - condensation of moisture on a cold surface\n",
      "None\n",
      "-----------------------------------\n",
      "Pun detected!\n",
      "Pun word: drill\n",
      "Selected senses for the word 'drill':\n",
      "drill (Similarity score: 0.6715) : a tool with a sharp point and cutting edges for making holes in hard materials (usually rotating rapidly or by repeated blows)\n",
      "drill (Similarity score: 0.6715) : similar to the mandrill but smaller and less brightly colored\n",
      "drill (Similarity score: 0.6715) : systematic training by multiple repetitions\n",
      "drill (Similarity score: 0.6715) : (military) the training of soldiers to march (as in ceremonial parades) or to perform the manual of arms\n",
      "drill (Similarity score: 0.6715) : make a hole, especially with a pointed power or hand tool\n",
      "drill (Similarity score: 0.6715) : train in the military, e.g., in the use of weapons\n",
      "drill (Similarity score: 0.6715) : learn by repetition\n",
      "drill (Similarity score: 0.6715) : teach by repetition\n",
      "drill (Similarity score: 0.6715) : undergo military training or do military exercises\n",
      "\n",
      "Sending multiple senses to LLM for verification...\n",
      "\n",
      "LLM's feedback on the chosen senses:\n",
      "1. drill - a tool with a sharp point and cutting edges for making holes in hard materials (usually rotating rapidly or by repeated blows)\n",
      "7. drill - learn by repetition\n",
      "None\n",
      "-----------------------------------\n",
      "Pun detected!\n",
      "Pun word: match\n",
      "Selected senses for the word 'match':\n",
      "match (Similarity score: 0.5600) : lighter consisting of a thin piece of wood or cardboard tipped with combustible chemical; ignites with friction\n",
      "match (Similarity score: 0.5600) : a formal contest in which two or more persons or teams compete\n",
      "match (Similarity score: 0.5600) : a burning piece of wood or cardboard\n",
      "match (Similarity score: 0.5600) : an exact duplicate\n",
      "match (Similarity score: 0.5600) : the score needed to win a match\n",
      "match (Similarity score: 0.5600) : a person regarded as a good matrimonial prospect\n",
      "match (Similarity score: 0.5600) : a person who is of equal standing with another in a group\n",
      "match (Similarity score: 0.5600) : a pair of people who live together\n",
      "match (Similarity score: 0.5600) : something that resembles or harmonizes with\n",
      "match (Similarity score: 0.5600) : be compatible, similar or consistent; coincide in their characteristics\n",
      "match (Similarity score: 0.5600) : provide funds complementary to\n",
      "match (Similarity score: 0.5600) : bring two objects, ideas, or people together\n",
      "match (Similarity score: 0.5600) : be equal to in quality or ability\n",
      "match (Similarity score: 0.5600) : make correspond or harmonize\n",
      "match (Similarity score: 0.5600) : satisfy or fulfill\n",
      "match (Similarity score: 0.5600) : give or join in marriage\n",
      "match (Similarity score: 0.5600) : set into opposition or rivalry\n",
      "match (Similarity score: 0.5600) : be equal or harmonize\n",
      "match (Similarity score: 0.5600) : make equal, uniform, corresponding, or matching\n",
      "\n",
      "Sending multiple senses to LLM for verification...\n",
      "\n",
      "LLM's feedback on the chosen senses:\n",
      "2. match - a formal contest in which two or more persons or teams compete\n",
      "17. match - set into opposition or rivalry\n",
      "None\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "sentences = [\"They hid from the gunman in a sauna where they could sweat it out.\",\n",
    "                     \"I've been to the dentist so many times, I really know the drill.\",\n",
    "                     \"A contest held by fire fighters is called a ' match '.\"\n",
    "                     ]\n",
    "for sentence in sentences:\n",
    "  pun_complete_detection_st_gpt(sentence)\n",
    "  print(\"-----------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "kuperman_aoa = pd.read_csv(\"AOA_adjusted_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_of_acquisition = {str(row['Word']).lower(): row['Rating.Mean'] for index, row in kuperman_aoa.iterrows() if isinstance(row['Word'], str)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_age_of_acquisition(word, age):\n",
    "    '''\n",
    "    This function checks if the word exists in the age of acquisition dataset\n",
    "    and if its Rating.Mean is equal to or less than the given age.\n",
    "    Returns True if the word is appropriate for the given age, False otherwise.\n",
    "    '''\n",
    "    word = word.lower()  # Ensure the word is in lowercase to match the dataset\n",
    "\n",
    "    # Try to get the AoA rating from the dictionary for the exact word\n",
    "    if word in age_of_acquisition:\n",
    "        rating_mean = age_of_acquisition[word]\n",
    "        return rating_mean <= age\n",
    "\n",
    "    # If not found, try lemmatizing the word\n",
    "    lemma = lemmatizer.lemmatize(word)\n",
    "    \n",
    "    # Check if the lemma has an AoA rating\n",
    "    if lemma in age_of_acquisition:\n",
    "        rating_mean = age_of_acquisition[lemma]\n",
    "        return rating_mean <= age\n",
    "    \n",
    "    # If word or its lemma is not found, return False\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function to check AoA for a word and its two senses\n",
    "def analyze_word_for_aoa(word, age):\n",
    "    '''\n",
    "    This function analyzes the input word, identifies its first two senses,\n",
    "    and checks if they are appropriate for the given age.\n",
    "    Returns a list containing the word, its senses, and confirmation of appropriateness.\n",
    "    '''\n",
    "    word = word.lower()\n",
    "    lemma = lemmatizer.lemmatize(word)\n",
    "    \n",
    "    # Get the first two senses of the word using WordNet\n",
    "    senses = wn.synsets(lemma)[:2]\n",
    "    \n",
    "    if senses:\n",
    "        # Initialize a list to store information about the word\n",
    "        word_info = {\n",
    "            'word': word,\n",
    "            'senses': [],\n",
    "        }\n",
    "        \n",
    "        # For each of the two senses, check the age of acquisition\n",
    "        for sense in senses:\n",
    "            sense_definition = sense.definition()  # Get the sense definition\n",
    "            aoa_appropriate = get_age_of_acquisition(lemma, age)  # Fetch AoA rating for the lemma\n",
    "            \n",
    "            # Add the sense and its AoA status to the word info\n",
    "            word_info['senses'].append({\n",
    "                'sense_definition': sense_definition,\n",
    "                'appropriate_for_age': aoa_appropriate\n",
    "            })\n",
    "        \n",
    "        # Return the results for the word\n",
    "        return word_info\n",
    "    else:\n",
    "        return None  # No senses found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pun_full_workflow(sentence,age):\n",
    "    if pun_detection(sentence) == 1:\n",
    "        print(\"Pun detected!\")\n",
    "        cleaned_sentence = preprocess_sentence(sentence)\n",
    "        pun_word = find_pun(cleaned_sentence)\n",
    "        print(\"Pun word:\", pun_word)\n",
    "        print(interpret_pun_with_gpt(cleaned_sentence, pun_word,2))\n",
    "        print('Age Appropriate for ' + str(age) + ': ' + str(get_age_of_acquisition(pun_word, age)))\n",
    "\n",
    "    else:\n",
    "        print(\"No pun detected in the sentence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pun detected!\n",
      "Pun word: sweat\n",
      "Selected senses for the word 'sweat':\n",
      "sweat (Similarity score: 0.5356) : salty fluid secreted by sweat glands\n",
      "sweat (Similarity score: 0.5356) : agitation resulting from active worry\n",
      "sweat (Similarity score: 0.5356) : condensation of moisture on a cold surface\n",
      "sweat (Similarity score: 0.5356) : use of physical or mental energy; hard work\n",
      "sweat (Similarity score: 0.5356) : excrete perspiration through the pores in the skin\n",
      "\n",
      "Sending multiple senses to LLM for verification...\n",
      "\n",
      "LLM's feedback on the chosen senses:\n",
      "The sentence is quite unclear, but the possible interpretations could be:\n",
      "1. sweat - salty fluid secreted by sweat glands\n",
      "5. sweat - excrete perspiration through the pores in the skin\n",
      "None\n",
      "Age Appropriate for 7: False\n",
      "-----------------------------------\n",
      "Pun detected!\n",
      "Pun word: drill\n",
      "Selected senses for the word 'drill':\n",
      "drill (Similarity score: 0.6715) : a tool with a sharp point and cutting edges for making holes in hard materials (usually rotating rapidly or by repeated blows)\n",
      "drill (Similarity score: 0.6715) : similar to the mandrill but smaller and less brightly colored\n",
      "drill (Similarity score: 0.6715) : systematic training by multiple repetitions\n",
      "drill (Similarity score: 0.6715) : (military) the training of soldiers to march (as in ceremonial parades) or to perform the manual of arms\n",
      "drill (Similarity score: 0.6715) : make a hole, especially with a pointed power or hand tool\n",
      "drill (Similarity score: 0.6715) : train in the military, e.g., in the use of weapons\n",
      "drill (Similarity score: 0.6715) : learn by repetition\n",
      "drill (Similarity score: 0.6715) : teach by repetition\n",
      "drill (Similarity score: 0.6715) : undergo military training or do military exercises\n",
      "\n",
      "Sending multiple senses to LLM for verification...\n",
      "\n",
      "LLM's feedback on the chosen senses:\n",
      "1. drill - a tool with a sharp point and cutting edges for making holes in hard materials (usually rotating rapidly or by repeated blows)\n",
      "3. drill - systematic training by multiple repetitions\n",
      "None\n",
      "Age Appropriate for 7: False\n",
      "-----------------------------------\n",
      "Pun detected!\n",
      "Pun word: match\n",
      "Selected senses for the word 'match':\n",
      "match (Similarity score: 0.5600) : lighter consisting of a thin piece of wood or cardboard tipped with combustible chemical; ignites with friction\n",
      "match (Similarity score: 0.5600) : a formal contest in which two or more persons or teams compete\n",
      "match (Similarity score: 0.5600) : a burning piece of wood or cardboard\n",
      "match (Similarity score: 0.5600) : an exact duplicate\n",
      "match (Similarity score: 0.5600) : the score needed to win a match\n",
      "match (Similarity score: 0.5600) : a person regarded as a good matrimonial prospect\n",
      "match (Similarity score: 0.5600) : a person who is of equal standing with another in a group\n",
      "match (Similarity score: 0.5600) : a pair of people who live together\n",
      "match (Similarity score: 0.5600) : something that resembles or harmonizes with\n",
      "match (Similarity score: 0.5600) : be compatible, similar or consistent; coincide in their characteristics\n",
      "match (Similarity score: 0.5600) : provide funds complementary to\n",
      "match (Similarity score: 0.5600) : bring two objects, ideas, or people together\n",
      "match (Similarity score: 0.5600) : be equal to in quality or ability\n",
      "match (Similarity score: 0.5600) : make correspond or harmonize\n",
      "match (Similarity score: 0.5600) : satisfy or fulfill\n",
      "match (Similarity score: 0.5600) : give or join in marriage\n",
      "match (Similarity score: 0.5600) : set into opposition or rivalry\n",
      "match (Similarity score: 0.5600) : be equal or harmonize\n",
      "match (Similarity score: 0.5600) : make equal, uniform, corresponding, or matching\n",
      "\n",
      "Sending multiple senses to LLM for verification...\n",
      "\n",
      "LLM's feedback on the chosen senses:\n",
      "1. match - a formal contest in which two or more persons or teams compete\n",
      "2. match - set into opposition or rivalry\n",
      "None\n",
      "Age Appropriate for 7: True\n",
      "-----------------------------------\n",
      "Pun detected!\n",
      "Pun word: technology\n",
      "Selected senses for the word 'technology':\n",
      "technology (Similarity score: 0.4463) : the practical application of science to commerce or industry\n",
      "technology (Similarity score: 0.4463) : the discipline dealing with the art or science of applying scientific knowledge to practical problems\n",
      "\n",
      "No need for LLM verification. Only two senses selected.\n",
      "None\n",
      "Age Appropriate for 7: False\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "sentences = [\"They hid from the gunman in a sauna where they could sweat it out.\",\n",
    "                     \"I've been to the dentist so many times, I really know the drill.\",\n",
    "                     \"A contest held by fire fighters is called a match.\", \"We are students of computer and information technology.\"\n",
    "                     ]\n",
    "for sentence in sentences:\n",
    "  pun_full_workflow(sentence,7)\n",
    "  print(\"-----------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pun_full_workflow(\"This is where I keep my arrows, said Tom, quivering\",15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pun_full_workflow(\"Why do elephants have a trunk? Because they don’t have pockets to put stuff in\",5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pun detected!\n",
      "Pun word: dessert\n",
      "Selected senses for the word 'dessert':\n",
      "dessert (Similarity score: 0.6268) : a dish served as the last course of a meal\n",
      "sweet (Similarity score: 0.1915) : a dish served as the last course of a meal\n",
      "\n",
      "No need for LLM verification. Only two senses selected.\n",
      "None\n",
      "Age Appropriate for 6: True\n"
     ]
    }
   ],
   "source": [
    "pun_full_workflow(\"The teddy bear was stuffed, so he said no to the dessert\",6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pun detected!\n",
      "Pun word: thoughtful\n",
      "Selected senses for the word 'thoughtful':\n",
      "thoughtful (Similarity score: 0.3732) : having intellectual depth\n",
      "thoughtful (Similarity score: 0.3732) : exhibiting or characterized by careful thought\n",
      "thoughtful (Similarity score: 0.3732) : acting with or showing thought and good sense\n",
      "thoughtful (Similarity score: 0.3732) : taking heed; giving close and thoughtful attention\n",
      "thoughtful (Similarity score: 0.3732) : considerate of the feelings or well-being of others\n",
      "\n",
      "Sending multiple senses to LLM for verification...\n",
      "\n",
      "LLM's feedback on the chosen senses:\n",
      "1. thoughtful - having intellectual depth\n",
      "2. thoughtful - exhibiting or characterized by careful thought\n",
      "None\n",
      "Age Appropriate for 4: False\n"
     ]
    }
   ],
   "source": [
    "pun_full_workflow(\"Why did the imagination go to school? To become a little more thoughtful.\",4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlt-project-1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
